{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설정값 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, config_path):\n",
    "        with open(config_path, 'r') as f:\n",
    "            self._yaml = f.read()\n",
    "            self._dict = yaml.load(self._yaml)\n",
    "            self._dict['PATH'] = os.path.dirname(config_path)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if self._dict.get(name) is not None:\n",
    "            return self._dict[name]\n",
    "\n",
    "        if DEFAULT_CONFIG.get(name) is not None:\n",
    "            return DEFAULT_CONFIG[name]\n",
    "\n",
    "        return None\n",
    "\n",
    "    def print(self):\n",
    "        print('Model configurations:')\n",
    "        print('---------------------------------')\n",
    "        print(self._yaml)\n",
    "        print('')\n",
    "        print('---------------------------------')\n",
    "        print('')\n",
    "        \n",
    "DEFAULT_CONFIG = {\n",
    "    'MODE': 1,                      # 1: train, 2: test, 3: eval\n",
    "    'MODEL': 1,                     # 1: edge model, 2: inpaint model, 3: edge-inpaint model, 4: joint model\n",
    "    'MASK': 3,                      # 1: random block, 2: half, 3: external, 4: (external, random block), 5: (external, random block, half)\n",
    "    'EDGE': 1,                      # 1: canny, 2: external\n",
    "    'NMS': 1,                       # 0: no non-max-suppression, 1: applies non-max-suppression on the external edges by multiplying by Canny\n",
    "    'SEED': 10,                     # random seed\n",
    "    'GPU': [0],                     # list of gpu ids\n",
    "    'DEBUG': 0,                     # turns on debugging mode\n",
    "    'VERBOSE': 0,                   # turns on verbose mode in the output console\n",
    "    \n",
    "    'LR': 0.0001,                   # learning rate\n",
    "    'D2G_LR': 0.1,                  # discriminator/generator learning rate ratio\n",
    "    'BETA1': 0.0,                   # adam optimizer beta1\n",
    "    'BETA2': 0.9,                   # adam optimizer beta2\n",
    "    'BATCH_SIZE': 8,                # input batch size for training\n",
    "    'INPUT_SIZE': 256,              # input image size for training 0 for original size\n",
    "    'SIGMA': 2,                     # standard deviation of the Gaussian filter used in Canny edge detector (0: random, -1: no edge)\n",
    "    'MAX_ITERS': 2e6,               # maximum number of iterations to train the model\n",
    "\n",
    "    'EDGE_THRESHOLD': 0.5,          # edge detection threshold\n",
    "    'L1_LOSS_WEIGHT': 1,            # l1 loss weight\n",
    "    'FM_LOSS_WEIGHT': 10,           # feature-matching loss weight\n",
    "    'STYLE_LOSS_WEIGHT': 1,         # style loss weight\n",
    "    'CONTENT_LOSS_WEIGHT': 1,       # perceptual loss weight\n",
    "    'INPAINT_ADV_LOSS_WEIGHT': 0.01,# adversarial loss weight\n",
    "\n",
    "    'GAN_LOSS': 'nsgan',            # nsgan | lsgan | hinge\n",
    "    'GAN_POOL_SIZE': 0,             # fake images pool size\n",
    "\n",
    "    'SAVE_INTERVAL': 1000,          # how many iterations to wait before saving model (0: never)\n",
    "    'SAMPLE_INTERVAL': 1000,        # how many iterations to wait before sampling (0: never)\n",
    "    'SAMPLE_SIZE': 12,              # number of images to sample\n",
    "    'EVAL_INTERVAL': 0,             # how many iterations to wait before model evaluation (0: never)\n",
    "    'LOG_INTERVAL': 10,             # how many iterations to wait before logging training status (0: never)\n",
    "    'SEG_NETWORK': 0,               # 0:DeepLabV3 resnet 101 segmentation , 1: MASK-R-CNN resnet 101 segmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 함수\n",
    "\n",
    "- data폴더 내에 있는 이미지 파일들로 Segmentation과 Inpainting을 수행할 예정이고, 두 과업은 각각 Pretrained된 모델을 가져와서 사용할 것이다. 또한 위에서 지정한 변수값들을 지정할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from imageio import imread\n",
    "from skimage.feature import canny\n",
    "from torch.utils.data import DataLoader\n",
    "from src.utils import create_mask\n",
    "from skimage.color import rgb2gray, gray2rgb\n",
    "import torchvision.transforms.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConnectDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, config, flist, edge_flist, augment = True, \n",
    "                 training = True):\n",
    "        super(EdgeConnectDataset, self).__init__()\n",
    "        self.augment = augment\n",
    "        self.training = training\n",
    "        self.data = self.load_flist(flist)\n",
    "        self.edge_data = self.load_flist(edge_flist)\n",
    "        \n",
    "        self.input_size = config.INPUT_SIZE\n",
    "        self.sigma = config.SIGMA\n",
    "        self.edge = config.EDGE\n",
    "        self.mask = config.MASK\n",
    "        self.nms = config.NMS\n",
    "        self.device = config.SEG_DEVICE\n",
    "        self.objects = config.OBJECTS\n",
    "        self.segment_net = config.SEG_NETWORK\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        try:\n",
    "            item = self.load_item(index)\n",
    "        except:\n",
    "            print('loading error : ' + self.data[index])\n",
    "            item = self.load_item(0)\n",
    "            \n",
    "        return item\n",
    "    \n",
    "    def load_item(self, index):\n",
    "        size = self.input_size\n",
    "        \n",
    "        # load image\n",
    "        img = Image.open(self.data[index])\n",
    "        \n",
    "        # gray to rgb\n",
    "        if img.mode != 'RGB':\n",
    "            img = gray2rgb(np.array(img))\n",
    "            img = Image.fromarray(img)\n",
    "            \n",
    "        # resize / crop if needed\n",
    "        img, mask = segmentor(self.segment_net, img, self.device, self.objects)\n",
    "        img = Image.fromarray(img)\n",
    "        img = np.array(img.resize((size, size), Image.ANTIALIAS))\n",
    "        \n",
    "        # create grayscale image\n",
    "        img_gray = rgb2gray(np.array(img))\n",
    "        \n",
    "        # load mask\n",
    "        mask = Image.fromarray(mask)\n",
    "        mask = np.array(mask.resize((size, size), Image.ANTIALIAS))\n",
    "        idx = (mask > 0)\n",
    "        mask[idx] = 255\n",
    "        mask = np.apply_along_axis(fill_gaps, 1, mask) # horizontal padding\n",
    "        mask = np.apply_along_axis(fill_gaps, 0, mask) # vertical padding\n",
    "        \n",
    "        # load edge\n",
    "        edge = self.load_edge(img_gray, index, mask)\n",
    "            \n",
    "        # augment data\n",
    "        if self.augment and np.random.binomial(1, 0.5) > 0:\n",
    "            img = img[:, ::-1, ...]\n",
    "            img_gray = img_gray[:, ::-1, ...]\n",
    "            edge = edge[:, ::-1, ...]\n",
    "            mask = mask[:, ::-1, ...]\n",
    "            \n",
    "        return self.to_tensor(img), self.to_tensor(img_gray), \\\n",
    "                self.to_tensor(edge), self.to_tensor(mask)\n",
    "    \n",
    "    def load_edge(self, img, index, mask):\n",
    "        sigma = self.sigma\n",
    "        \n",
    "        '''\n",
    "        In test mode, images are masked(with masked regions),\n",
    "        using 'mask' parameter prevents canny to detect edges for the masked\n",
    "        regions\n",
    "        '''\n",
    "        mask = None if self.training else (1 - mask / 255).astype(np.bool)\n",
    "        \n",
    "        # canny\n",
    "        if self.edge == 1:\n",
    "            # no edge\n",
    "            if sigma == -1:\n",
    "                return np.zeros(img.shape).astype(np.float)\n",
    "            \n",
    "            # random sigma\n",
    "            if sigma == 0:\n",
    "                sigma = random.randint(1, 4)\n",
    "                \n",
    "            return canny(img, sigma = sigma, mask = mask).astype(np.float)\n",
    "        # external\n",
    "        else:\n",
    "            img_h, img_w = img.shapepe[:2]\n",
    "            edge = imread(self.edge_data[index])\n",
    "            edge = self.resized(edge, img_h, img_w)\n",
    "            \n",
    "            # Non-Max Suppression\n",
    "            if self.nms == 1:\n",
    "                edge = edge * canny(img, sigma = sigma, mask = mask)\n",
    "                \n",
    "            return edge\n",
    "        \n",
    "    \n",
    "    def to_tensor(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img_t = F.to_tensor(img).float()\n",
    "        \n",
    "        return img_t\n",
    "    \n",
    "    def load_flist(self, flist):\n",
    "        if isinstance(flist, list):\n",
    "            return flist\n",
    "        \n",
    "        # flist : image file path, image directory path, text file flist path\n",
    "        if isinstance(flist, str):\n",
    "            if os.path.isdir(flist):\n",
    "                flist = list(glob.glob(flist + '/.jpg'))\n",
    "                flist.sort()\n",
    "                \n",
    "                return flist\n",
    "            \n",
    "            if os.path.isfile(flist):\n",
    "                try:\n",
    "                    return np.genfromtxt(flist, dtype = np.str, \n",
    "                                         encoding = 'utf-8')\n",
    "                except:\n",
    "                    return [flist]\n",
    "                \n",
    "        return []\n",
    "    \n",
    "    def create_iterator(self, batch_size):\n",
    "        while True:\n",
    "            sample_loader = DataLoader(\n",
    "                dataset = self,\n",
    "                batch_size = batch_size,\n",
    "                drop_last = True\n",
    "            )\n",
    "            \n",
    "            for item in sample_loader:\n",
    "                yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation 수행\n",
    "- Segmentation에는 Semantic Segmentation과 Instance Segmentation이 있다.\n",
    "- Semantic Segmentation은 이미지 내 모든 Object들에 대해 Instance(person1, person2, ...)를 구분하지 않고 특정 class에 속하는 Object들 모두 같게 Segmentation을 수행한다.\n",
    "- Instance Segmentation은 이미지 내 모든 Object들에 대해 Instance를 구분해서 특정 class에 속하는 Object들 중 Instance별로 다르게 Segmentation을 수행하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation 모델\n",
    "- pretrained된 모델로 FCN(Fully Convolution Network)과 DeeplabV3모델을 사용해서 Semantic Segmentation을 수행할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import Progbar, create_dir, stitch_images, imsave\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn = models.segmentation.fcn_resnet101(pretrained = True).eval()\n",
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained = True).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Segmentation 모델\n",
    "- pretrained된 모델로 Mask-R-CNN모델을 사용해서 Instance Segmentation을 수행할 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_r_cnn = models.detection.maskrcnn_resnet50_fpn(pretrained = 1, \n",
    "                                                    progress = False).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(image,objects,nc=21):\n",
    "                \n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in objects:\n",
    "        idx = image == l\n",
    "        r[idx] = 255#fill  r with 255 wherever class is 1 and so on\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps(values):\n",
    "    searchval=[255,0,255]\n",
    "    searchval2=[255,0,0,255]\n",
    "    idx=(np.array(np.where((values[:-2]==searchval[0]) & (values[1:-1]==searchval[1]) & (values[2:]==searchval[2])))+1)\n",
    "    idx2=(np.array(np.where((values[:-3]==searchval2[0]) & (values[1:-2]==searchval2[1]) & (values[2:-1]==searchval2[2]) & (values[3:]==searchval2[3])))+1)\n",
    "    idx3=(idx2+1)\n",
    "    new=idx.tolist()+idx2.tolist()+idx3.tolist()\n",
    "    newlist = [item for items in new for item in items]\n",
    "    values[newlist]=255\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps2(values):\n",
    "    searchval=[0,255]\n",
    "    searchval2=[255,0]\n",
    "    idx=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1]))))\n",
    "    idx2=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1])))+1)\n",
    "    \n",
    "    new=idx.tolist()+idx2.tolist()\n",
    "    newlist = [item for items in new for item in items]\n",
    "    values[newlist]=255\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_gaps2(values):\n",
    "    searchval=[0,255]\n",
    "    searchval2=[255,0]\n",
    "    idx=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1]))))\n",
    "    idx2=(np.array(np.where((values[:-1]==searchval[0]) & (values[1:]==searchval[1])))+1)\n",
    "    \n",
    "    new=idx.tolist()+idx2.tolist()\n",
    "    newlist = [item for items in new for item in items]\n",
    "    values[newlist]=255\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 전처리\n",
    "- 테스트 이미지이지만 데이터 수가 적기 때문에 데이터 확대 기법을 적용해서 많은 이미지로 과업들을 수행할 수 있도록 하려 했으나 pretrained된 모델로 과업을 수행할 계획이라 전처리 기법을 많이 적용하지 않을 예정이다.\n",
    "- PyTorch에서 제공하는 transforms모듈 내에서 Resize, ToTensor, Normalize함수를 이용해 테스트 이미지들에 대한 전처리를 수행한다.\n",
    "- 백지연"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentor(seg_net,img,dev,objects):\n",
    "    #plt.imshow(img); plt.show()\n",
    "    if seg_net == 1:\n",
    "        net = mask_r_cnn\n",
    "    else:\n",
    "        net = dlab\n",
    "    if dev == 'cuda':\n",
    "        trf = T.Compose([T.Resize(400),\n",
    "                         T.ToTensor(), \n",
    "                         T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                    std = [0.229, 0.224, 0.225])])\n",
    "    else:\n",
    "        trf = T.Compose([T.Resize(680),\n",
    "                         T.ToTensor(), \n",
    "                         T.Normalize(mean = [0.485, 0.456, 0.406], \n",
    "                                     std = [0.229, 0.224, 0.225])])\n",
    "        \n",
    "    inp = trf(img).unsqueeze(0).to(dev)\n",
    "    out = net.to(dev)(inp)['out']\n",
    "    om = torch.argmax(out.squeeze(), dim = 0).detach().cpu().numpy()\n",
    "    mask = decode_segmap(om,objects)\n",
    "    height,width = mask.shape\n",
    "    img = np.array(img.resize((width, height), Image.ANTIALIAS))\n",
    "\n",
    "\n",
    "    og_img = remove_patch_og(img,mask)\n",
    "    plt.imshow(mask); plt.show()\n",
    "    \n",
    "    return og_img,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rudck\\anaconda3\\envs\\inpainting\\lib\\site-packages\\ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# 설정값 파일 경로\n",
    "config_path = os.path.join('./checkpoints', 'config.yml')\n",
    "\n",
    "# 설정값 파일 가져오기\n",
    "config = Config(config_path)\n",
    "\n",
    "# 전체 테스트 데이터셋\n",
    "test_dataset = EdgeConnectDataset(config, config.TEST_FLIST, config.TEST_EDGE_FLIST,\n",
    "                            augment = False, training = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수값 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\train_1.jpg' 'data\\\\cat_2.jpg' 'data\\\\cat_4.jpg']\n"
     ]
    }
   ],
   "source": [
    "test_batch_size = 1\n",
    "\n",
    "no_cuda = False\n",
    "\n",
    "# 전체 테스트 데이터 저장경로\n",
    "test_data_paths = glob.glob('data/*.jpg')\n",
    "\n",
    "# 랜덤 인덱스 이용해 랜덤 이미지로 과업 수행\n",
    "rnd_test_datas = np.random.choice(test_data_paths, 3)\n",
    "print(rnd_test_datas)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_lst = transforms.Compose([\n",
    "    transforms.Resize(400),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(path):\n",
    "    return os.path.basename(path).split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['TVmonitor', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle',\n",
       "       'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n",
       "       'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train'],\n",
       "      dtype='<U11')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = [get_label(path) for path in test_data_paths]\n",
    "classes = np.unique(label_names)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_paths, transform = None):\n",
    "        self.data_paths = data_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        path = self.data_paths[idx]\n",
    "        \n",
    "        # Read image\n",
    "        image_pil = Image.open(path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image_pil)\n",
    "            \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "kwargs = {'num_workers' : 0, 'pin_memory' : True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    customDataset(rnd_test_datas, transforms_lst),\n",
    "    batch_size = test_batch_size,\n",
    "    shuffle = False,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcn = models.segmentation.fcn_resnet101(pretrained = True).eval()\n",
    "dlab = models.segmentation.deeplabv3_resnet101(pretrained = True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\train_1.jpg'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = rnd_test_datas[random.randrange(0, len(rnd_test_datas))]\n",
    "image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682, 1024, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_pil = Image.open(image_path)\n",
    "image = np.array(image_pil)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# input_image = transforms_lst(image_pil).unsqueeze(0).to(device)\n",
    "# output = fcn.to(device)(input_image)['out']\n",
    "# output_max = torch.argmax(output.squeeze(), dim = 0).detach().cpu().numpy()\n",
    "\n",
    "# plt.imshow(output_max)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rudck\\anaconda3\\envs\\inpainting\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_image = transforms_lst(image_pil).unsqueeze(0).to(device)\n",
    "output = dlab.to(device)(input_image)['out']\n",
    "output_max = torch.argmax(output.squeeze(), dim = 0).detach().cpu().numpy()\n",
    "\n",
    "plt.imshow(output_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rudck\\anaconda3\\envs\\inpainting\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12736/2875283700.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransforms_lst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_pil\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask_r_cnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'out'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moutput_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "input_image = transforms_lst(image_pil).unsqueeze(0).to(device)\n",
    "output = mask_r_cnn.to(device)(input_image)['out']\n",
    "output_max = torch.argmax(output.squeeze(), dim = 0).detach().cpu().numpy()\n",
    "\n",
    "plt.imshow(output_max)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_segmap(image,objects,nc=21):\n",
    "                \n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in objects:\n",
    "        idx = image == l\n",
    "        r[idx] = 255#fill  r with 255 wherever class is 1 and so on\n",
    "    return np.array(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_patch_og(real_img,mask):\n",
    "    og_data = real_img.copy()\n",
    "    idx = mask == 255  ### cutting out mask part from real image here\n",
    "    og_data[idx] =255\n",
    "    return og_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_classes = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "mask_r_cnn = maskrcnn_resnet50_fpn(pretrained = True, progress = False)\n",
    "mask_r_cnn = mask_r_cnn.eval()\n",
    "\n",
    "output = mask_r_cnn(batch)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "inst_class_to_idx = {cls : idx for (idx, cls) in enumerate(inst_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_output, bicycle_output = output[0], output[1]\n",
    "person_masks, bicycle_masks = person_output['masks'], bicycle_output['masks']\n",
    "print(f'shape = {person_masks.shape}, dtype = {bicycle_masks.dtype}, '\n",
    "      f'min = {person_masks.min()}, max = {bicycle_masks.max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "inst_class_to_idx = {cls : idx for (idx, cls) in enumerate(inst_classes)}\n",
    "\n",
    "print('For the first person, the following instances were detected : ')\n",
    "print([inst_classes[label] for label in person_output['labels']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_threshold = 0.5\n",
    "person_bool_masks = person_output['masks'] > proba_threshold\n",
    "print(f'shape = {person_bool_masks.shape}, dtype = {person_bool_masks.dtype}')\n",
    "\n",
    "person_bool_masks = person_bool_masks.squeeze(1)\n",
    "\n",
    "show(draw_segmentation_masks(person1_int, person_bool_masks, alpha = 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle_bool_masks = bicycle_output['masks'] > proba_threshold\n",
    "print(f'shape = {bicycle_bool_masks.shape}, dtype = {bicycle_bool_masks.dtype}')\n",
    "\n",
    "bicycle_bool_masks = bicycle_bool_masks.squeeze(1)\n",
    "\n",
    "show(draw_segmentation_masks(bicycle1_int, bicycle_bool_masks, alpha = 0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inpainting 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
